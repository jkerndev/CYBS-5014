{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5b713a3",
   "metadata": {},
   "source": [
    "# The Parser and Analysis Module for DNS TunViz: research on a DNS tunneling detection visualization tool ready for SOCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8c06792b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "# PCAP Analysis\n",
    "from scapy.all import rdpcap, DNS, DNSQR, DNSRR\n",
    "\n",
    "# Data Analysis\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Data Modeling/Processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    f1_score\n",
    ")\n",
    "\n",
    "# Data Visualization\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bdd94e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "# CIC-Bell-DNS-EXF-2021 dataset used for this research: https://www.unb.ca/cic/datasets/dns-exf-2021.html\n",
    "# Citation: Samaneh Mahdavifar, Amgad Hanafy Salem, Princy Victor, Miguel Garzon, Amir H. Razavi, Natasha Hellberg, Arash Habibi Lashkari, “Lightweight Hybrid Detection of Data Exfiltration using DNS based on Machine Learning”, The 11th IEEE International Conference on Communication and Network Security (ICCNS), Dec. 3-5, 2021, Beijing Jiaotong University, Weihai, China.\n",
    "\n",
    "BENIGN_PCAP_DIR = \"CICBellEXFDNS2021/PCAP/All_benign\"        # Directory containing benign PCAP files\n",
    "MALICIOUS_PCAP_DIR = \"CICBellEXFDNS2021/PCAP/All_malicious\"  # Directory containing malicious PCAP files\n",
    "OUTPUT_FILE = \"dns_features.json\"                            # Output file for extracted features   \n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5cc418",
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "\n",
    "Extracts features related to DNS tunneling:\n",
    "\n",
    "1) Length of full domain\n",
    "2) Longest subdomain value\n",
    "3) Number of unique subdomains per apex domain\n",
    "4) Shannon entropy of subdomain\n",
    "5) n-gram frequency vectors of subdomains\n",
    "6) Mean and variance of request size\n",
    "7) RR-type freqneices: Per-domain frequencies of A/AAAA/TXT/NULL/CNAME/MX/NS/PTR records\n",
    "8) Mean and variance of TTLs over responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2445ed17",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNSFeatureExtractor:\n",
    "    def __init__(self, ngram_range: Tuple[int, int] = (2, 3), max_features: int = 100):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.vectorizer = CountVectorizer(\n",
    "            analyzer='char',\n",
    "            ngram_range=ngram_range,\n",
    "            max_features=max_features\n",
    "        )\n",
    "        self.rr_types = ['A', 'AAAA', 'TXT', 'NULL', 'CNAME', 'MX', 'NS', 'PTR']\n",
    "        \n",
    "    def extract_subdomains(self, domain: str) -> List[str]:\n",
    "        parts = domain.rstrip('.').split('.')\n",
    "        if len(parts) <= 2:\n",
    "            return []\n",
    "        return parts[:-2]\n",
    "    \n",
    "    def calculate_shannon_entropy(self, text: str) -> float:\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        counts = Counter(text)\n",
    "        probabilities = [count / len(text) for count in counts.values()]\n",
    "        return entropy(probabilities, base=2)\n",
    "    \n",
    "    def extract_features_from_pcap(self, pcap_path: str) -> List[Dict]:\n",
    "        packets = rdpcap(pcap_path)\n",
    "        \n",
    "        domain_data = defaultdict(lambda: {\n",
    "            'queries': [],\n",
    "            'subdomains': [],\n",
    "            'request_sizes': [],\n",
    "            'rr_counts': Counter(),\n",
    "            'ttls': []\n",
    "        })\n",
    "        \n",
    "        for pkt_num, packet in enumerate(tqdm(packets, desc=\"  Processing packets\", leave=False)):\n",
    "            try:\n",
    "                if not packet.haslayer(DNS):\n",
    "                    continue\n",
    "                    \n",
    "                dns = packet[DNS]\n",
    "                \n",
    "                if dns.qd:\n",
    "                    try:\n",
    "                        if hasattr(dns.qd, 'qname'):\n",
    "                            qname_raw = dns.qd.qname\n",
    "                            if isinstance(qname_raw, bytes):\n",
    "                                qname = qname_raw.decode('utf-8', errors='ignore')\n",
    "                            else:\n",
    "                                qname = str(qname_raw)\n",
    "                            \n",
    "                            if not qname or qname == '.':\n",
    "                                continue\n",
    "                            \n",
    "                            parts = qname.rstrip('.').split('.')\n",
    "                            if len(parts) < 2:\n",
    "                                continue\n",
    "                            \n",
    "                            apex = '.'.join(parts[-2:])\n",
    "                            \n",
    "                            domain_data[apex]['queries'].append(qname)\n",
    "                            domain_data[apex]['request_sizes'].append(len(packet))\n",
    "                            \n",
    "                            subdomains = self.extract_subdomains(qname)\n",
    "                            domain_data[apex]['subdomains'].extend(subdomains)\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f\"  Warning: Packet {pkt_num} query parsing failed: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if dns.an:\n",
    "                    try:\n",
    "                        an_records = dns.an if isinstance(dns.an, list) else [dns.an]\n",
    "                        for rr in an_records:\n",
    "                            if hasattr(rr, 'rrname'):\n",
    "                                rrname_raw = rr.rrname\n",
    "                                if isinstance(rrname_raw, bytes):\n",
    "                                    rrname = rrname_raw.decode('utf-8', errors='ignore')\n",
    "                                else:\n",
    "                                    rrname = str(rrname_raw)\n",
    "                                \n",
    "                                parts = rrname.rstrip('.').split('.')\n",
    "                                parts = [p for p in parts if p]  # Remove empty parts\n",
    "                                if len(parts) < 2:\n",
    "                                    continue\n",
    "                                    \n",
    "                                apex = '.'.join(parts[-2:])\n",
    "                                rr_type = self.get_rr_type(rr.type)\n",
    "                                domain_data[apex]['rr_counts'][rr_type] += 1\n",
    "                                \n",
    "                                if hasattr(rr, 'ttl'):\n",
    "                                    domain_data[apex]['ttls'].append(rr.ttl)\n",
    "                    except Exception as e:\n",
    "                        tqdm.write(f\"  Warning: Packet {pkt_num} answer parsing failed: {e}\")\n",
    "                        continue\n",
    "                        \n",
    "            except Exception as e:\n",
    "                tqdm.write(f\"  Warning: Packet {pkt_num} processing failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return self.compute_features(domain_data)\n",
    "    \n",
    "    def extract_features_with_timestamps(self, pcap_path: str) -> Tuple[List[Dict], List[datetime]]:\n",
    "        \"\"\"Extract features and timestamps from PCAP.\"\"\"\n",
    "        packets = rdpcap(pcap_path)\n",
    "        \n",
    "        domain_data = defaultdict(lambda: {\n",
    "            'queries': [], 'subdomains': [], 'request_sizes': [],\n",
    "            'rr_counts': Counter(), 'ttls': [], 'timestamps': []\n",
    "        })\n",
    "        \n",
    "        for packet in tqdm(packets, desc=\"Processing packets\"):\n",
    "            try:\n",
    "                if not packet.haslayer(DNS):\n",
    "                    continue\n",
    "                \n",
    "                dns = packet[DNS]\n",
    "                timestamp = datetime.fromtimestamp(float(packet.time))\n",
    "                \n",
    "                if dns.qd and hasattr(dns.qd, 'qname'):\n",
    "                    qname_raw = dns.qd.qname\n",
    "                    qname = qname_raw.decode('utf-8', errors='ignore') if isinstance(qname_raw, bytes) else str(qname_raw)\n",
    "                    \n",
    "                    if qname and qname != '.':\n",
    "                        parts = qname.rstrip('.').split('.')\n",
    "                        if len(parts) >= 2:\n",
    "                            apex = '.'.join(parts[-2:])\n",
    "                            domain_data[apex]['queries'].append(qname)\n",
    "                            domain_data[apex]['request_sizes'].append(len(packet))\n",
    "                            domain_data[apex]['timestamps'].append(timestamp)\n",
    "                            domain_data[apex]['subdomains'].extend(self.extract_subdomains(qname))\n",
    "                \n",
    "                if dns.an:\n",
    "                    an_records = dns.an if isinstance(dns.an, list) else [dns.an]\n",
    "                    for rr in an_records:\n",
    "                        if hasattr(rr, 'rrname'):\n",
    "                            rrname_raw = rr.rrname\n",
    "                            rrname = rrname_raw.decode('utf-8', errors='ignore') if isinstance(rrname_raw, bytes) else str(rrname_raw)\n",
    "                            parts = [p for p in rrname.rstrip('.').split('.') if p]\n",
    "                            if len(parts) >= 2:\n",
    "                                apex = '.'.join(parts[-2:])\n",
    "                                domain_data[apex]['rr_counts'][self.get_rr_type(rr.type)] += 1\n",
    "                                if hasattr(rr, 'ttl'):\n",
    "                                    domain_data[apex]['ttls'].append(rr.ttl)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        features = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for apex, data in domain_data.items():\n",
    "            if data['queries']:\n",
    "                feature_dict = {\n",
    "                    'apex_domain': apex,\n",
    "                    'avg_domain_length': float(np.mean([len(q) for q in data['queries']])),\n",
    "                    'max_domain_length': int(max([len(q) for q in data['queries']])),\n",
    "                    'longest_subdomain': int(max([len(s) for s in data['subdomains']])) if data['subdomains'] else 0,\n",
    "                    'unique_subdomain_count': int(len(set(data['subdomains']))),\n",
    "                    'avg_subdomain_entropy': float(np.mean([self.calculate_shannon_entropy(s) for s in data['subdomains']])) if data['subdomains'] else 0.0,\n",
    "                    'request_size_mean': float(np.mean(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                    'request_size_var': float(np.var(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                    'ttl_mean': float(np.mean(data['ttls'])) if data['ttls'] else 0.0,\n",
    "                    'ttl_var': float(np.var(data['ttls'])) if data['ttls'] else 0.0,\n",
    "                }\n",
    "                \n",
    "                total_rr = sum(data['rr_counts'].values())\n",
    "                for rr_type in self.rr_types:\n",
    "                    feature_dict[f'rr_freq_{rr_type}'] = float(data['rr_counts'][rr_type] / total_rr if total_rr > 0 else 0)\n",
    "                \n",
    "                features.append(feature_dict)\n",
    "                timestamps.append(min(data['timestamps']) if data['timestamps'] else datetime.now())\n",
    "        \n",
    "        return features, timestamps\n",
    "    \n",
    "    def get_rr_type(self, type_code: int) -> str:\n",
    "        type_map = {1: 'A', 28: 'AAAA', 16: 'TXT', 10: 'NULL', \n",
    "                    5: 'CNAME', 15: 'MX', 2: 'NS', 12: 'PTR'}\n",
    "        return type_map.get(type_code, 'OTHER')\n",
    "    \n",
    "    def compute_features(self, domain_data: Dict) -> Dict:\n",
    "        features = []\n",
    "        \n",
    "        for apex, data in domain_data.items():\n",
    "            if not data['queries']:\n",
    "                continue\n",
    "            \n",
    "            feature_dict = {\n",
    "                'apex_domain': apex,\n",
    "                'avg_domain_length': float(np.mean([len(q) for q in data['queries']])),\n",
    "                'max_domain_length': int(max([len(q) for q in data['queries']])),\n",
    "                'longest_subdomain': int(max([len(s) for s in data['subdomains']])) if data['subdomains'] else 0,\n",
    "                'unique_subdomain_count': int(len(set(data['subdomains']))),\n",
    "                'avg_subdomain_entropy': float(np.mean([\n",
    "                    self.calculate_shannon_entropy(s) for s in data['subdomains']\n",
    "                ])) if data['subdomains'] else 0.0,\n",
    "                'request_size_mean': float(np.mean(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                'request_size_var': float(np.var(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                'ttl_mean': float(np.mean(data['ttls'])) if data['ttls'] else 0.0,\n",
    "                'ttl_var': float(np.var(data['ttls'])) if data['ttls'] else 0.0,\n",
    "            }\n",
    "            \n",
    "            total_rr = sum(data['rr_counts'].values())\n",
    "            for rr_type in self.rr_types:\n",
    "                feature_dict[f'rr_freq_{rr_type}'] = float(\n",
    "                    data['rr_counts'][rr_type] / total_rr if total_rr > 0 else 0\n",
    "                )\n",
    "            \n",
    "            features.append(feature_dict)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def add_ngram_features(self, features: List[Dict]) -> List[Dict]:\n",
    "        all_subdomains = []\n",
    "        subdomain_indices = []\n",
    "        \n",
    "        for idx, feature in enumerate(features):\n",
    "            apex = feature['apex_domain']\n",
    "            subdomain_text = apex.replace('.', '')\n",
    "            all_subdomains.append(subdomain_text)\n",
    "            subdomain_indices.append(idx)\n",
    "        \n",
    "        if all_subdomains:\n",
    "            ngram_matrix = self.vectorizer.fit_transform(all_subdomains).toarray()\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            \n",
    "            for idx, ngram_vec in zip(subdomain_indices, ngram_matrix):\n",
    "                for fname, value in zip(feature_names, ngram_vec):\n",
    "                    features[idx][f'ngram_{fname}'] = int(value)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def process_directory(self, pcap_dir: str, label: str) -> List[Dict]:\n",
    "        all_features = []\n",
    "        \n",
    "        pcap_files = list(Path(pcap_dir).glob('*.pcap*'))\n",
    "        \n",
    "        for pcap_file in tqdm(pcap_files, desc=f\"Processing {label} PCAPs\"):\n",
    "            try:\n",
    "                features = self.extract_features_from_pcap(str(pcap_file))\n",
    "                if features:\n",
    "                    for feature in features:\n",
    "                        feature['label'] = label\n",
    "                        feature['source_file'] = pcap_file.name\n",
    "                    all_features.extend(features)\n",
    "                    tqdm.write(f\"  {pcap_file.name}: Extracted {len(features)} domain features\")\n",
    "                else:\n",
    "                    tqdm.write(f\"  {pcap_file.name}: No valid DNS features found\")\n",
    "            except Exception as e:\n",
    "                import traceback\n",
    "                tqdm.write(f\"Error processing {pcap_file}: {e}\")\n",
    "                tqdm.write(traceback.format_exc())\n",
    "        \n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ed32a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE EXTRACTION\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs:  17%|█▋        | 1/6 [00:38<03:14, 38.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign_2.pcap: Extracted 15324 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs:  33%|███▎      | 2/6 [01:40<03:28, 52.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign_1.pcap: Extracted 21629 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs:  50%|█████     | 3/6 [02:14<02:11, 43.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign_heavy_3.pcap: Extracted 12120 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs:  67%|██████▋   | 4/6 [02:40<01:13, 36.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign_heavy_2.pcap: Extracted 8691 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs:  83%|████████▎ | 5/6 [03:08<00:33, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign.pcap: Extracted 10365 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing benign PCAPs: 100%|██████████| 6/6 [03:37<00:00, 36.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  benign_heavy_1.pcap: Extracted 10390 domain features\n",
      "\n",
      "Benign features extracted: 78519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:   8%|▊         | 1/12 [00:34<06:20, 34.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_exe.pcap: Extracted 16 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  17%|█▋        | 2/12 [00:37<02:40, 16.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_video.pcap: Extracted 8 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  25%|██▌       | 3/12 [00:39<01:26,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_text.pcap: Extracted 9 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  33%|███▎      | 4/12 [00:53<01:29, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_compressed.pcap: Extracted 13 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  42%|████▏     | 5/12 [01:04<01:18, 11.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_audio.pcap: Extracted 12 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  50%|█████     | 6/12 [01:40<01:57, 19.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_audio.pcap: Extracted 15 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  58%|█████▊    | 7/12 [01:49<01:20, 16.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_exe.pcap: Extracted 10 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  67%|██████▋   | 8/12 [02:21<01:24, 21.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_compressed.pcap: Extracted 17 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  75%|███████▌  | 9/12 [02:21<00:44, 14.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  light_image.pcap: Extracted 4 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  83%|████████▎ | 10/12 [03:16<00:54, 27.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_text.pcap: Extracted 20 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs:  92%|█████████▏| 11/12 [03:58<00:31, 31.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_image.pcap: Extracted 14 domain features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing malicious PCAPs: 100%|██████████| 12/12 [04:37<00:00, 23.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  heavy_video.pcap: Extracted 15 domain features\n",
      "Malicious features extracted: 153\n",
      "\n",
      "Combining and adding n-gram features...\n",
      "Saving 78672 total features to dns_features.json...\n",
      "\n",
      "============================================================\n",
      "COMPLETE - Extracted 78519 benign and 153 malicious samples\n",
      "Features saved to dns_features.json\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "extractor = DNSFeatureExtractor(ngram_range=(2, 3), max_features=50)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FEATURE EXTRACTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "benign_features = extractor.process_directory(BENIGN_PCAP_DIR, label='benign')\n",
    "print(f\"\\nBenign features extracted: {len(benign_features)}\")\n",
    "\n",
    "malicious_features = extractor.process_directory(MALICIOUS_PCAP_DIR, label='malicious')\n",
    "print(f\"Malicious features extracted: {len(malicious_features)}\")\n",
    "\n",
    "print(\"\\nCombining and adding n-gram features...\")\n",
    "all_features = benign_features + malicious_features\n",
    "all_features = extractor.add_ngram_features(all_features)\n",
    "\n",
    "print(f\"Saving {len(all_features)} total features to {OUTPUT_FILE}...\")\n",
    "with open(OUTPUT_FILE, 'w') as f:\n",
    "    json.dump(all_features, f, indent=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"COMPLETE - Extracted {len(benign_features)} benign and {len(malicious_features)} malicious samples\")\n",
    "print(f\"Features saved to {OUTPUT_FILE}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c194ba",
   "metadata": {},
   "source": [
    "# Model creation\n",
    "\n",
    "Between SVM and Random Forests being the most accurate models for this kind of data, this research chooses to use a Random Forest due to its easier implementation and interpretability.\n",
    "\n",
    "Below is the process used for modelling. Steps:\n",
    "\n",
    "1) Loading the features from the json file; extracted from above\n",
    "2) Data split into training and evaluation data sets; model trained and evaluated\n",
    "3) Plotting visualizations for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "389bdef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNSTunnelingDetector:\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.random_state = random_state\n",
    "        self.model: RandomForestClassifier = None\n",
    "        self.scaler: StandardScaler = StandardScaler()\n",
    "        self.feature_names: List[str] = None\n",
    "        self.label_mapping = {\"benign\": 0, \"malicious\": 1}\n",
    "        \n",
    "    def load_data(self, json_file_path: str) -> pd.DataFrame:\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(f\"Loaded {len(df)} samples from {json_file_path}\")\n",
    "        print(f\"Label distribution:\\n{df['label'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        exclude_columns = ['apex_domain', 'source_file', 'label']\n",
    "        feature_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "        \n",
    "        self.feature_names = feature_columns\n",
    "        \n",
    "        X = df[feature_columns].values\n",
    "        y = df['label'].map(self.label_mapping).values\n",
    "        \n",
    "        print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "        print(f\"Number of features: {len(feature_columns)}\")\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray,\n",
    "        optimize_hyperparameters: bool = False\n",
    "    ) -> None:\n",
    "        X_train_scaled = self.scaler.fit_transform(X_train)\n",
    "        \n",
    "        if optimize_hyperparameters:\n",
    "            print(\"\\nOptimizing hyperparameters...\")\n",
    "            self.model = self._optimize_hyperparameters(X_train_scaled, y_train)\n",
    "        else:\n",
    "            self.model = RandomForestClassifier(\n",
    "                n_estimators=100,\n",
    "                max_depth=20,\n",
    "                min_samples_split=5,\n",
    "                min_samples_leaf=2,\n",
    "                max_features='sqrt',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1,\n",
    "                class_weight='balanced'\n",
    "            )\n",
    "            self.model.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        print(\"\\nTraining completed!\")\n",
    "        print(f\"Number of trees: {self.model.n_estimators}\")\n",
    "        print(f\"Max depth: {self.model.max_depth}\")\n",
    "\n",
    "    def balance_dataset(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        downsample_ratio: float = 0.1,\n",
    "        random_state: int = None\n",
    "    ) -> pd.DataFrame:\n",
    "        if random_state is None:\n",
    "            random_state = self.random_state\n",
    "        \n",
    "        benign_df = df[df['label'] == 'benign']\n",
    "        malicious_df = df[df['label'] == 'malicious']\n",
    "        \n",
    "        print(f\"\\nOriginal dataset distribution:\")\n",
    "        print(f\"  Benign: {len(benign_df)}\")\n",
    "        print(f\"  Malicious: {len(malicious_df)}\")\n",
    "        print(f\"  Ratio (benign:malicious): {len(benign_df)/len(malicious_df):.1f}:1\")\n",
    "        \n",
    "        benign_sample_size = int(len(benign_df) * downsample_ratio)\n",
    "        benign_downsampled = benign_df.sample(\n",
    "            n=benign_sample_size,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        \n",
    "        balanced_df = pd.concat([benign_downsampled, malicious_df], ignore_index=True)\n",
    "        balanced_df = balanced_df.sample(frac=1, random_state=random_state).reset_index(drop=True)\n",
    "        \n",
    "        print(f\"\\nBalanced dataset distribution:\")\n",
    "        print(f\"  Benign: {len(benign_downsampled)}\")\n",
    "        print(f\"  Malicious: {len(malicious_df)}\")\n",
    "        print(f\"  Ratio (benign:malicious): {len(benign_downsampled)/len(malicious_df):.1f}:1\")\n",
    "        print(f\"  Total samples: {len(balanced_df)}\")\n",
    "        print(f\"  Reduction: {(1 - len(balanced_df)/len(df)) * 100:.1f}%\")\n",
    "        \n",
    "        return balanced_df\n",
    "    \n",
    "    def _optimize_hyperparameters(\n",
    "        self,\n",
    "        X_train: np.ndarray,\n",
    "        y_train: np.ndarray\n",
    "    ) -> RandomForestClassifier:\n",
    "        param_grid = {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "        rf = RandomForestClassifier(\n",
    "            random_state=self.random_state,\n",
    "            n_jobs=-1,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            rf,\n",
    "            param_grid,\n",
    "            cv=5,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "        print(f\"Best F1 score: {grid_search.best_score_:.4f}\")\n",
    "        \n",
    "        return grid_search.best_estimator_\n",
    "    \n",
    "    def evaluate(\n",
    "        self,\n",
    "        X_test: np.ndarray,\n",
    "        y_test: np.ndarray\n",
    "    ) -> Dict[str, Any]:\n",
    "        X_test_scaled = self.scaler.transform(X_test)\n",
    "        \n",
    "        y_pred = self.model.predict(X_test_scaled)\n",
    "        y_pred_proba = self.model.predict_proba(X_test_scaled)[:, 1]\n",
    "        \n",
    "        results = {\n",
    "            'classification_report': classification_report(\n",
    "                y_test,\n",
    "                y_pred,\n",
    "                target_names=['benign', 'malicious']\n",
    "            ),\n",
    "            'confusion_matrix': confusion_matrix(y_test, y_pred),\n",
    "            'roc_auc': roc_auc_score(y_test, y_pred_proba),\n",
    "            'f1_score': f1_score(y_test, y_pred),\n",
    "            'y_pred': y_pred,\n",
    "            'y_pred_proba': y_pred_proba\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"EVALUATION RESULTS\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"\\nROC-AUC Score: {results['roc_auc']:.4f}\")\n",
    "        print(f\"F1 Score: {results['f1_score']:.4f}\")\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(results['classification_report'])\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(results['confusion_matrix'])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cross_validate(self, X: np.ndarray, y: np.ndarray, cv: int = 5) -> None:\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        \n",
    "        scores = cross_val_score(\n",
    "            self.model,\n",
    "            X_scaled,\n",
    "            y,\n",
    "            cv=cv,\n",
    "            scoring='f1',\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCross-validation F1 scores: {scores}\")\n",
    "        print(f\"Mean F1: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    def get_feature_importance(self, top_n: int = 20) -> pd.DataFrame:\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': self.feature_names,\n",
    "            'importance': self.model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop {top_n} Most Important Features:\")\n",
    "        print(importance_df.head(top_n).to_string(index=False))\n",
    "        \n",
    "        return importance_df\n",
    "    \n",
    "    def plot_results(\n",
    "        self,\n",
    "        y_test: np.ndarray,\n",
    "        y_pred_proba: np.ndarray,\n",
    "        importance_df: pd.DataFrame,\n",
    "        output_dir: str = \"./outputs\"\n",
    "    ) -> None:\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Calculate metrics for plotting\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)\n",
    "        top_features = importance_df.head(15)\n",
    "\n",
    "        # Plot 1: ROC Curve (individual)\n",
    "        fig1, ax1 = plt.subplots(figsize=(8, 6))\n",
    "        ax1.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')\n",
    "        ax1.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        ax1.set_xlabel('False Positive Rate')\n",
    "        ax1.set_ylabel('True Positive Rate')\n",
    "        ax1.set_title('ROC Curve')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/roc_curve.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig1)\n",
    "\n",
    "        # Plot 2: Precision-Recall Curve (individual)\n",
    "        fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "        ax2.plot(recall, precision)\n",
    "        ax2.set_xlabel('Recall')\n",
    "        ax2.set_ylabel('Precision')\n",
    "        ax2.set_title('Precision-Recall Curve')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/precision_recall_curve.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig2)\n",
    "\n",
    "        # Plot 3: Feature Importance (individual)\n",
    "        fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
    "        ax3.barh(range(len(top_features)), top_features['importance'])\n",
    "        ax3.set_yticks(range(len(top_features)))\n",
    "        ax3.set_yticklabels(top_features['feature'])\n",
    "        ax3.set_xlabel('Importance')\n",
    "        ax3.set_title('Top 15 Feature Importances')\n",
    "        ax3.invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/feature_importance.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig3)\n",
    "\n",
    "        # Plot 4: Prediction Distribution (individual)\n",
    "        fig4, ax4 = plt.subplots(figsize=(8, 6))\n",
    "        ax4.hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, label='Benign')\n",
    "        ax4.hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, label='Malicious')\n",
    "        ax4.set_xlabel('Predicted Probability')\n",
    "        ax4.set_ylabel('Frequency')\n",
    "        ax4.set_title('Prediction Distribution')\n",
    "        ax4.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/prediction_distribution.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig4)\n",
    "\n",
    "        # Combined plot (all 4 plots together)\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "        # ROC Curve\n",
    "        axes[0, 0].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc:.4f})')\n",
    "        axes[0, 0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        axes[0, 0].set_xlabel('False Positive Rate')\n",
    "        axes[0, 0].set_ylabel('True Positive Rate')\n",
    "        axes[0, 0].set_title('ROC Curve')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        axes[0, 1].plot(recall, precision)\n",
    "        axes[0, 1].set_xlabel('Recall')\n",
    "        axes[0, 1].set_ylabel('Precision')\n",
    "        axes[0, 1].set_title('Precision-Recall Curve')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        # Feature Importance\n",
    "        axes[1, 0].barh(range(len(top_features)), top_features['importance'])\n",
    "        axes[1, 0].set_yticks(range(len(top_features)))\n",
    "        axes[1, 0].set_yticklabels(top_features['feature'])\n",
    "        axes[1, 0].set_xlabel('Importance')\n",
    "        axes[1, 0].set_title('Top 15 Feature Importances')\n",
    "        axes[1, 0].invert_yaxis()\n",
    "\n",
    "        # Prediction Distribution\n",
    "        axes[1, 1].hist(y_pred_proba[y_test == 0], bins=50, alpha=0.5, label='Benign')\n",
    "        axes[1, 1].hist(y_pred_proba[y_test == 1], bins=50, alpha=0.5, label='Malicious')\n",
    "        axes[1, 1].set_xlabel('Predicted Probability')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].set_title('Prediction Distribution')\n",
    "        axes[1, 1].legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/dns_tunneling_results.png\", dpi=300, bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "\n",
    "        print(f\"\\nPlots saved to {output_dir}:\")\n",
    "        print(f\"  - Combined: dns_tunneling_results.png\")\n",
    "        print(f\"  - Individual: roc_curve.png\")\n",
    "        print(f\"  - Individual: precision_recall_curve.png\")\n",
    "        print(f\"  - Individual: feature_importance.png\")\n",
    "        print(f\"  - Individual: prediction_distribution.png\")\n",
    "    \n",
    "    def save_model(self, model_path: str = \"./outputs/dns_model.pkl\") -> None:\n",
    "        Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        model_data = {\n",
    "            'model': self.model,\n",
    "            'scaler': self.scaler,\n",
    "            'feature_names': self.feature_names,\n",
    "            'label_mapping': self.label_mapping\n",
    "        }\n",
    "        \n",
    "        joblib.dump(model_data, model_path)\n",
    "        print(f\"\\nModel saved to {model_path}\")\n",
    "    \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        model_data = joblib.load(model_path)\n",
    "        \n",
    "        self.model = model_data['model']\n",
    "        self.scaler = model_data['scaler']\n",
    "        self.feature_names = model_data['feature_names']\n",
    "        self.label_mapping = model_data['label_mapping']\n",
    "        \n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        probabilities = self.model.predict_proba(X_scaled)[:, 1]\n",
    "        \n",
    "        return predictions, probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55efc977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 78672 samples from dns_features.json\n",
      "Label distribution:\n",
      "label\n",
      "benign       78519\n",
      "malicious      153\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Original dataset distribution:\n",
      "  Benign: 78519\n",
      "  Malicious: 153\n",
      "  Ratio (benign:malicious): 513.2:1\n",
      "\n",
      "Balanced dataset distribution:\n",
      "  Benign: 7851\n",
      "  Malicious: 153\n",
      "  Ratio (benign:malicious): 51.3:1\n",
      "  Total samples: 8004\n",
      "  Reduction: 89.8%\n",
      "\n",
      "Feature matrix shape: (8004, 67)\n",
      "Number of features: 67\n",
      "\n",
      "Training set size: 6403\n",
      "Test set size: 1601\n",
      "\n",
      "Training completed!\n",
      "Number of trees: 100\n",
      "Max depth: 20\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ROC-AUC Score: 0.9984\n",
      "F1 Score: 0.9032\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       1.00      1.00      1.00      1570\n",
      "   malicious       0.90      0.90      0.90        31\n",
      "\n",
      "    accuracy                           1.00      1601\n",
      "   macro avg       0.95      0.95      0.95      1601\n",
      "weighted avg       1.00      1.00      1.00      1601\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1567    3]\n",
      " [   3   28]]\n",
      "\n",
      "Top 20 Most Important Features:\n",
      "               feature  importance\n",
      "             rr_freq_A    0.222847\n",
      "      request_size_var    0.087998\n",
      " avg_subdomain_entropy    0.086642\n",
      "     max_domain_length    0.085639\n",
      "     avg_domain_length    0.067095\n",
      "     request_size_mean    0.058919\n",
      "              ttl_mean    0.047599\n",
      "         rr_freq_CNAME    0.042878\n",
      "     longest_subdomain    0.034737\n",
      "              ngram_ic    0.025691\n",
      "unique_subdomain_count    0.024664\n",
      "               ttl_var    0.021142\n",
      "              ngram_al    0.020948\n",
      "              ngram_ma    0.015375\n",
      "              ngram_ch    0.012425\n",
      "              ngram_le    0.010663\n",
      "              ngram_in    0.008784\n",
      "              ngram_or    0.007136\n",
      "              ngram_ac    0.006904\n",
      "              ngram_ne    0.006780\n",
      "\n",
      "Plots saved to ./outputs:\n",
      "  - Combined: dns_tunneling_results.png\n",
      "  - Individual: roc_curve.png\n",
      "  - Individual: precision_recall_curve.png\n",
      "  - Individual: feature_importance.png\n",
      "  - Individual: prediction_distribution.png\n",
      "\n",
      "Model saved to ./outputs/dns_model.pkl\n",
      "\n",
      "============================================================\n",
      "Training pipeline completed successfully!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "detector = DNSTunnelingDetector(random_state=42)\n",
    "\n",
    "df = detector.load_data(OUTPUT_FILE)\n",
    "\n",
    "df = detector.balance_dataset(df, downsample_ratio=0.1)\n",
    "\n",
    "X, y = detector.prepare_features(df)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")\n",
    "\n",
    "detector.train(X_train, y_train, optimize_hyperparameters=False)\n",
    "\n",
    "results = detector.evaluate(X_test, y_test)\n",
    "\n",
    "importance_df = detector.get_feature_importance(top_n=20)\n",
    "\n",
    "detector.plot_results(y_test, results['y_pred_proba'], importance_df)\n",
    "\n",
    "detector.save_model()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training pipeline completed successfully!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e425e21",
   "metadata": {},
   "source": [
    "# TunViz\n",
    "\n",
    "TunViz is a Dashboard which can do the following tasks:\n",
    "\n",
    "- Intake a new PCAP file and obtain the DNS queries for feature extraction and analysis\n",
    "- Analyze all relevant DNS queries using a previously trained Random Forest Classifier model to flag for benign or malicious DNS queries\n",
    "- Create a table for each and every query, allowing the user to filter the data down by time of the query and by type of data present in each field\n",
    "- Allow users to click on each entry in the table and display a SHAP Force Plot that intuitively allows someone with little machine learning or statistical background to understand why the model made the binary decision it made for that particular query\n",
    "- For all queries in the filtered-down table, create a parallel sets (alluvial) plot that shows how for each entry the features extracted and the final decision. The categories will be based on the features and the categories will be based on the threshold in the random forest that made that specific tree vote the way it did. For example, for the trees that evaluate the avg_domain_length for their decision, find the threshold that makes it vote \"benign\" or \"malicious\" and route the query through the corresponding category in the parallel sets (alluvial) plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNSDashboard:\n",
    "    def __init__(self, model_path: str = \"./outputs/dns_model.pkl\"):\n",
    "        \"\"\"Initialize the DNS Dashboard with model and feature extractor.\"\"\"\n",
    "        self.model_data = None\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "        # Load model\n",
    "        self.load_model(model_path)\n",
    "        \n",
    "        # Initialize feature extractor\n",
    "        self.extractor = DNSFeatureExtractor()\n",
    "        \n",
    "        # Data storage\n",
    "        self.current_data = None\n",
    "        self.filtered_data = None\n",
    "        self.shap_explainer = None\n",
    "        self.shap_values = None\n",
    "        \n",
    "    def load_model(self, model_path: str) -> None:\n",
    "        \"\"\"Load the trained model.\"\"\"\n",
    "        self.model_data = joblib.load(model_path)\n",
    "        self.model = self.model_data['model']\n",
    "        self.scaler = self.model_data['scaler']\n",
    "        self.feature_names = self.model_data['feature_names']\n",
    "        print(f\"Model loaded from {model_path}\")\n",
    "    \n",
    "    def analyze_pcap(self, pcap_path: str) -> pd.DataFrame:\n",
    "        \"\"\"Analyze a PCAP file and return results.\"\"\"\n",
    "        print(\"Extracting DNS features...\")\n",
    "        features, timestamps = self.extractor.extract_features_with_timestamps(pcap_path)\n",
    "        \n",
    "        if not features:\n",
    "            print(\"No DNS queries found\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        print(f\"Found {len(features)} DNS queries\")\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(features)\n",
    "        df['timestamp'] = timestamps\n",
    "        \n",
    "        # Add n-gram features\n",
    "        print(\"Adding n-gram features...\")\n",
    "        df = self.add_ngram_features_to_df(df)\n",
    "        \n",
    "        # Make predictions\n",
    "        print(\"Running predictions...\")\n",
    "        X = df[self.feature_names].values\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        predictions = self.model.predict(X_scaled)\n",
    "        probabilities = self.model.predict_proba(X_scaled)\n",
    "        \n",
    "        df['prediction'] = ['Malicious' if p == 1 else 'Benign' for p in predictions]\n",
    "        df['malicious_prob'] = probabilities[:, 1]\n",
    "        \n",
    "        # Calculate SHAP values\n",
    "        print(\"Computing SHAP explanations...\")\n",
    "        self.compute_shap_values(X_scaled)\n",
    "        \n",
    "        self.current_data = df\n",
    "        self.filtered_data = df.copy()\n",
    "        \n",
    "        print(\"Analysis complete!\")\n",
    "        return df\n",
    "    \n",
    "    def add_ngram_features_to_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add n-gram features to DataFrame.\"\"\"\n",
    "        for feature in self.feature_names:\n",
    "            if feature.startswith('ngram_') and feature not in df.columns:\n",
    "                df[feature] = 0\n",
    "        \n",
    "        all_features = df.to_dict('records')\n",
    "        enhanced_features = self.extractor.add_ngram_features(all_features)\n",
    "        \n",
    "        for i, feature_dict in enumerate(enhanced_features):\n",
    "            for key, value in feature_dict.items():\n",
    "                if key.startswith('ngram_'):\n",
    "                    df.at[i, key] = value\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def compute_shap_values(self, X_scaled: np.ndarray) -> None:\n",
    "        \"\"\"Compute SHAP values.\"\"\"\n",
    "        self.shap_explainer = shap.TreeExplainer(self.model)\n",
    "        self.shap_values = self.shap_explainer.shap_values(X_scaled)\n",
    "        \n",
    "        if isinstance(self.shap_values, list):\n",
    "            self.shap_values = self.shap_values[1]\n",
    "    \n",
    "    def create_interactive_table(self, max_rows: int = 20) -> widgets.VBox:\n",
    "        \"\"\"Create an interactive table with SHAP explanations.\"\"\"\n",
    "        if self.filtered_data is None:\n",
    "            return widgets.VBox([widgets.HTML(\"No data available\")])\n",
    "        \n",
    "        # Summary stats\n",
    "        total = len(self.filtered_data)\n",
    "        malicious = len(self.filtered_data[self.filtered_data['prediction'] == 'Malicious'])\n",
    "        benign = total - malicious\n",
    "        \n",
    "        stats_html = f\"\"\"\n",
    "        <div style=\"background: #f0f0f0; padding: 15px; border-radius: 8px; margin-bottom: 20px;\">\n",
    "            <h3>Summary Statistics</h3>\n",
    "            <div style=\"display: grid; grid-template-columns: repeat(3, 1fr); gap: 20px;\">\n",
    "                <div>\n",
    "                    <strong>Total Queries:</strong> {total}\n",
    "                </div>\n",
    "                <div>\n",
    "                    <strong>Malicious:</strong> <span style=\"color: red;\">{malicious} ({malicious/total*100:.1f}%)</span>\n",
    "                </div>\n",
    "                <div>\n",
    "                    <strong>Benign:</strong> <span style=\"color: green;\">{benign} ({benign/total*100:.1f}%)</span>\n",
    "                </div>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create table\n",
    "        display_cols = ['timestamp', 'apex_domain', 'prediction', 'malicious_prob']\n",
    "        table_data = self.filtered_data[display_cols].head(max_rows).copy()\n",
    "        table_data['malicious_prob'] = table_data['malicious_prob'].round(3)\n",
    "        \n",
    "        table_html = table_data.to_html(classes='table', index=True)\n",
    "        \n",
    "        # Create output for SHAP plots\n",
    "        shap_output = widgets.Output()\n",
    "        \n",
    "        # Index selector\n",
    "        index_selector = widgets.IntSlider(\n",
    "            value=0,\n",
    "            min=0,\n",
    "            max=min(len(self.filtered_data)-1, max_rows-1),\n",
    "            description='Query Index:',\n",
    "            continuous_update=False\n",
    "        )\n",
    "        \n",
    "        def update_shap(change):\n",
    "            with shap_output:\n",
    "                clear_output(wait=True)\n",
    "                self.display_shap_force_plot(self.filtered_data.index[change.new])\n",
    "        \n",
    "        index_selector.observe(update_shap, 'value')\n",
    "        \n",
    "        # Initial SHAP plot\n",
    "        with shap_output:\n",
    "            self.display_shap_force_plot(self.filtered_data.index[0])\n",
    "        \n",
    "        return widgets.VBox([\n",
    "            widgets.HTML(stats_html),\n",
    "            widgets.HTML(\"<h4>DNS Queries Table</h4>\"),\n",
    "            widgets.HTML(table_html),\n",
    "            widgets.HTML(\"<h4>SHAP Explanation (adjust slider to select query)</h4>\"),\n",
    "            index_selector,\n",
    "            shap_output\n",
    "        ])\n",
    "    \n",
    "    def display_shap_force_plot(self, index: int) -> None:\n",
    "        \"\"\"Display SHAP force plot for a specific query.\"\"\"\n",
    "        original_idx = self.current_data.index.get_loc(index)\n",
    "        shap_vals = self.shap_values[original_idx]\n",
    "        # Handle 2D SHAP values (binary classification)\n",
    "        if len(shap_vals.shape) > 1 and shap_vals.shape[1] == 2:\n",
    "            shap_vals = shap_vals[:, 1]  # Use malicious class SHAP values\n",
    "        feature_vals = self.current_data.loc[index, self.feature_names].values\n",
    "        \n",
    "        # Get top features\n",
    "        abs_shap = np.abs(shap_vals)\n",
    "        sorted_idx = np.argsort(abs_shap)[::-1][:10]\n",
    "        sorted_idx = list(sorted_idx.flat)\n",
    "        \n",
    "        # Use numpy indexing to extract arrays, then convert to lists\n",
    "        shap_values_sorted = shap_vals[sorted_idx].ravel().tolist()\n",
    "        feature_values_sorted = feature_vals[sorted_idx].ravel().tolist()\n",
    "        \n",
    "        # Extract feature names one by one\n",
    "        feature_names_sorted = []\n",
    "        for i in range(len(sorted_idx)):\n",
    "            idx_val = sorted_idx[i] \n",
    "            feature_names_sorted.append(str(self.feature_names[idx_val]))\n",
    "        \n",
    "        labels = [f\"{name} = {val:.2f}\" for name, val in zip(feature_names_sorted, feature_values_sorted)]\n",
    "        colors = ['red' if val > 0 else 'green' for val in shap_values_sorted]\n",
    "        \n",
    "        # Create plot\n",
    "        fig = go.Figure()\n",
    "        \n",
    "        fig.add_trace(go.Bar(\n",
    "            x=shap_values_sorted,\n",
    "            y=labels,\n",
    "            orientation='h',\n",
    "            marker_color=colors,\n",
    "            text=[f\"{val:.3f}\" for val in shap_values_sorted],\n",
    "            textposition='outside'\n",
    "        ))\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=f\"SHAP Explanation - {self.current_data.loc[index, 'apex_domain']}<br>\"\n",
    "                  f\"Prediction: {self.current_data.loc[index, 'prediction']} \"\n",
    "                  f\"(Prob: {self.current_data.loc[index, 'malicious_prob']:.3f})\",\n",
    "            xaxis_title=\"Feature Impact\",\n",
    "            yaxis_title=\"Features\",\n",
    "            height=400,\n",
    "            showlegend=False\n",
    "        )\n",
    "        \n",
    "        fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\")\n",
    "        fig.show()\n",
    "    \n",
    "    def create_parallel_sets(self) -> go.Figure:\n",
    "        \"\"\"Create parallel sets visualization.\"\"\"\n",
    "        if self.filtered_data is None or len(self.filtered_data) == 0:\n",
    "            return go.Figure()\n",
    "        \n",
    "        # Sample data if too large\n",
    "        sample_data = self.filtered_data.sample(min(100, len(self.filtered_data)))\n",
    "        \n",
    "        # Categorize features\n",
    "        categories = []\n",
    "        \n",
    "        try:\n",
    "            # Domain length - use duplicates='drop' to handle edge cases\n",
    "            length_bins = pd.qcut(sample_data['avg_domain_length'], q=3, labels=['Short', 'Medium', 'Long'], duplicates='drop')\n",
    "            categories.append({'label': 'Domain Length', 'values': length_bins.astype(str).values})\n",
    "        except ValueError:\n",
    "            # If qcut fails, use simple cut or direct categorization\n",
    "            median_length = sample_data['avg_domain_length'].median()\n",
    "            length_bins = pd.cut(sample_data['avg_domain_length'], \n",
    "                                bins=[-np.inf, median_length*0.8, median_length*1.2, np.inf], \n",
    "                                labels=['Short', 'Medium', 'Long'])\n",
    "            categories.append({'label': 'Domain Length', 'values': length_bins.astype(str).values})\n",
    "        \n",
    "        try:\n",
    "            # Entropy - use duplicates='drop' to handle edge cases\n",
    "            entropy_bins = pd.qcut(sample_data['avg_subdomain_entropy'], q=3, labels=['Low', 'Medium', 'High'], duplicates='drop')\n",
    "            categories.append({'label': 'Entropy', 'values': entropy_bins.astype(str).values})\n",
    "        except ValueError:\n",
    "            # If qcut fails, use simple categorization\n",
    "            median_entropy = sample_data['avg_subdomain_entropy'].median()\n",
    "            entropy_bins = pd.cut(sample_data['avg_subdomain_entropy'], \n",
    "                                 bins=[-np.inf, median_entropy*0.8, median_entropy*1.2, np.inf], \n",
    "                                 labels=['Low', 'Medium', 'High'])\n",
    "            categories.append({'label': 'Entropy', 'values': entropy_bins.astype(str).values})\n",
    "        \n",
    "        # Subdomains\n",
    "        subdomain_bins = pd.cut(sample_data['unique_subdomain_count'], \n",
    "                                bins=[-1, 0, 5, float('inf')], \n",
    "                                labels=['None', 'Few', 'Many'])\n",
    "        categories.append({'label': 'Subdomains', 'values': subdomain_bins.astype(str).values})\n",
    "        \n",
    "        # Prediction\n",
    "        categories.append({'label': 'Prediction', 'values': sample_data['prediction'].values})\n",
    "        \n",
    "        # Create plot\n",
    "        fig = go.Figure(data=[go.Parcats(\n",
    "            dimensions=categories,\n",
    "            line={'color': sample_data['malicious_prob'], \n",
    "                  'colorscale': 'RdYlGn_r',\n",
    "                  'showscale': True,\n",
    "                  'colorbar': {'title': 'Malicious<br>Probability'}},\n",
    "            hoveron='color',\n",
    "            hoverinfo='count+probability'\n",
    "        )])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title=\"Feature Flow to Prediction (Parallel Sets)\",\n",
    "            height=500\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "\n",
    "\n",
    "class DNSFeatureExtractor2:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the feature extractor.\"\"\"\n",
    "        self.rr_types = ['A', 'AAAA', 'TXT', 'NULL', 'CNAME', 'MX', 'NS', 'PTR']\n",
    "        self.ngram_features = [\n",
    "            'ngram_ac', 'ngram_al', 'ngram_am', 'ngram_an', 'ngram_ar', 'ngram_as', 'ngram_at',\n",
    "            'ngram_ch', 'ngram_co', 'ngram_com', 'ngram_de', 'ngram_ec', 'ngram_eco', 'ngram_ed',\n",
    "            'ngram_el', 'ngram_en', 'ngram_er', 'ngram_es', 'ngram_et', 'ngram_ic', 'ngram_in',\n",
    "            'ngram_it', 'ngram_la', 'ngram_le', 'ngram_li', 'ngram_ma', 'ngram_me', 'ngram_nc',\n",
    "            'ngram_ne', 'ngram_net', 'ngram_ng', 'ngram_nt', 'ngram_om', 'ngram_on', 'ngram_or',\n",
    "            'ngram_ra', 'ngram_rc', 'ngram_re', 'ngram_ri', 'ngram_ro', 'ngram_ru', 'ngram_sc',\n",
    "            'ngram_sco', 'ngram_se', 'ngram_st', 'ngram_ta', 'ngram_tc', 'ngram_te', 'ngram_ti', 'ngram_to'\n",
    "        ]\n",
    "    \n",
    "    def extract_subdomains(self, domain: str) -> List[str]:\n",
    "        parts = domain.rstrip('.').split('.')\n",
    "        return parts[:-2] if len(parts) > 2 else []\n",
    "    \n",
    "    def calculate_shannon_entropy(self, text: str) -> float:\n",
    "        if not text:\n",
    "            return 0.0\n",
    "        counts = Counter(text)\n",
    "        probabilities = [count / len(text) for count in counts.values()]\n",
    "        return entropy(probabilities, base=2)\n",
    "    \n",
    "    def get_rr_type(self, type_code: int) -> str:\n",
    "        type_map = {1: 'A', 28: 'AAAA', 16: 'TXT', 10: 'NULL', \n",
    "                    5: 'CNAME', 15: 'MX', 2: 'NS', 12: 'PTR'}\n",
    "        return type_map.get(type_code, 'OTHER')\n",
    "    \n",
    "    def extract_features_with_timestamps(self, pcap_path: str) -> Tuple[List[Dict], List[datetime]]:\n",
    "        \"\"\"Extract features and timestamps from PCAP.\"\"\"\n",
    "        packets = rdpcap(pcap_path)\n",
    "        \n",
    "        domain_data = defaultdict(lambda: {\n",
    "            'queries': [], 'subdomains': [], 'request_sizes': [],\n",
    "            'rr_counts': Counter(), 'ttls': [], 'timestamps': []\n",
    "        })\n",
    "        \n",
    "        for packet in tqdm(packets, desc=\"Processing packets\"):\n",
    "            try:\n",
    "                if not packet.haslayer(DNS):\n",
    "                    continue\n",
    "                \n",
    "                dns = packet[DNS]\n",
    "                timestamp = datetime.fromtimestamp(float(packet.time))\n",
    "                \n",
    "                if dns.qd and hasattr(dns.qd, 'qname'):\n",
    "                    qname_raw = dns.qd.qname\n",
    "                    qname = qname_raw.decode('utf-8', errors='ignore') if isinstance(qname_raw, bytes) else str(qname_raw)\n",
    "                    \n",
    "                    if qname and qname != '.':\n",
    "                        parts = qname.rstrip('.').split('.')\n",
    "                        if len(parts) >= 2:\n",
    "                            apex = '.'.join(parts[-2:])\n",
    "                            domain_data[apex]['queries'].append(qname)\n",
    "                            domain_data[apex]['request_sizes'].append(len(packet))\n",
    "                            domain_data[apex]['timestamps'].append(timestamp)\n",
    "                            domain_data[apex]['subdomains'].extend(self.extract_subdomains(qname))\n",
    "                \n",
    "                if dns.an:\n",
    "                    an_records = dns.an if isinstance(dns.an, list) else [dns.an]\n",
    "                    for rr in an_records:\n",
    "                        if hasattr(rr, 'rrname'):\n",
    "                            rrname_raw = rr.rrname\n",
    "                            rrname = rrname_raw.decode('utf-8', errors='ignore') if isinstance(rrname_raw, bytes) else str(rrname_raw)\n",
    "                            parts = [p for p in rrname.rstrip('.').split('.') if p]\n",
    "                            if len(parts) >= 2:\n",
    "                                apex = '.'.join(parts[-2:])\n",
    "                                domain_data[apex]['rr_counts'][self.get_rr_type(rr.type)] += 1\n",
    "                                if hasattr(rr, 'ttl'):\n",
    "                                    domain_data[apex]['ttls'].append(rr.ttl)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        features = []\n",
    "        timestamps = []\n",
    "        \n",
    "        for apex, data in domain_data.items():\n",
    "            if data['queries']:\n",
    "                feature_dict = {\n",
    "                    'apex_domain': apex,\n",
    "                    'avg_domain_length': float(np.mean([len(q) for q in data['queries']])),\n",
    "                    'max_domain_length': int(max([len(q) for q in data['queries']])),\n",
    "                    'longest_subdomain': int(max([len(s) for s in data['subdomains']])) if data['subdomains'] else 0,\n",
    "                    'unique_subdomain_count': int(len(set(data['subdomains']))),\n",
    "                    'avg_subdomain_entropy': float(np.mean([self.calculate_shannon_entropy(s) for s in data['subdomains']])) if data['subdomains'] else 0.0,\n",
    "                    'request_size_mean': float(np.mean(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                    'request_size_var': float(np.var(data['request_sizes'])) if data['request_sizes'] else 0.0,\n",
    "                    'ttl_mean': float(np.mean(data['ttls'])) if data['ttls'] else 0.0,\n",
    "                    'ttl_var': float(np.var(data['ttls'])) if data['ttls'] else 0.0,\n",
    "                }\n",
    "                \n",
    "                total_rr = sum(data['rr_counts'].values())\n",
    "                for rr_type in self.rr_types:\n",
    "                    feature_dict[f'rr_freq_{rr_type}'] = float(data['rr_counts'][rr_type] / total_rr if total_rr > 0 else 0)\n",
    "                \n",
    "                features.append(feature_dict)\n",
    "                timestamps.append(min(data['timestamps']) if data['timestamps'] else datetime.now())\n",
    "        \n",
    "        return features, timestamps\n",
    "    \n",
    "    def add_ngram_features(self, features: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Add n-gram features.\"\"\"\n",
    "        for feature in features:\n",
    "            apex = feature['apex_domain']\n",
    "            subdomain_text = apex.replace('.', '')\n",
    "            \n",
    "            ngrams = set()\n",
    "            for n in [2, 3]:\n",
    "                for i in range(len(subdomain_text) - n + 1):\n",
    "                    ngrams.add(subdomain_text[i:i+n])\n",
    "            \n",
    "            for ngram_feature in self.ngram_features:\n",
    "                ngram = ngram_feature.replace('ngram_', '')\n",
    "                feature[ngram_feature] = 1 if ngram in ngrams else 0\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa31628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TunViz: DNS Tunneling Detection Dashboard\n",
      "==================================================\n",
      "Model loaded from ./outputs/dns_model.pkl\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8ebbe45a914e12a22ed58faead74d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>Step 1: Upload PCAP File</h2>'), FileUpload(value=(), accept='.pcap,.pcapng', d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dashboard ready! Upload a PCAP file to start.\n"
     ]
    }
   ],
   "source": [
    "print(\"TunViz: DNS Tunneling Detection Dashboard\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize dashboard\n",
    "dashboard = DNSDashboard(model_path=\"./outputs/dns_model.pkl\")\n",
    "\n",
    "# File upload widget\n",
    "upload_widget = widgets.FileUpload(\n",
    "    accept='.pcap,.pcapng',\n",
    "    multiple=False,\n",
    "    description='Upload PCAP:'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def process_upload(change):\n",
    "    \"\"\"Process uploaded file.\"\"\"\n",
    "    with output_area:\n",
    "        clear_output(wait=True)\n",
    "        \n",
    "        if upload_widget.value:\n",
    "            # Save uploaded file temporarily\n",
    "            # upload_widget.value is a tuple of uploaded file info\n",
    "            uploaded_file_info = upload_widget.value[0]\n",
    "            temp_path = Path('/tmp/temp_dns_analysis.pcap')\n",
    "            temp_path.write_bytes(uploaded_file_info['content'])\n",
    "            \n",
    "            print(f\"Uploaded file: {uploaded_file_info['name']}\")\n",
    "            \n",
    "            # Analyze\n",
    "            df = dashboard.analyze_pcap(str(temp_path))\n",
    "            \n",
    "            if not df.empty:\n",
    "                # Create interactive table\n",
    "                table_widget = dashboard.create_interactive_table()\n",
    "                display(table_widget)\n",
    "                \n",
    "                # Create parallel sets plot\n",
    "                print(\"\\nParallel Sets Visualization:\")\n",
    "                try:\n",
    "                    fig = dashboard.create_parallel_sets()\n",
    "                    display(fig)\n",
    "                except Exception as e:\n",
    "                    print(f\"Note: Could not display parallel sets plot. Error: {e}\")\n",
    "                    print(\"The interactive table with SHAP explanations is still available above.\")\n",
    "\n",
    "upload_widget.observe(process_upload, names='value')\n",
    "\n",
    "# Display UI\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>Step 1: Upload PCAP File</h2>\"),\n",
    "    upload_widget,\n",
    "    widgets.HTML(\"<p style='color: gray;'>Upload a PCAP file to begin analysis</p>\"),\n",
    "    output_area\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tunviz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
